# -*- coding: utf-8 -*-
"""skillsExtraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SmJDwHq8MP09QYyni_64q4pecMjU-Brb
"""
import spacy
from spacy.pipeline import EntityRuler
from spacy.lang.en import English
from spacy.tokens import Doc
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity


#gensim
# import gensim
# from gensim import corpora



#Data loading/ Data manipulation
import pandas as pd
import numpy as np
# !pip install jsonlines
import jsonlines

#nltk
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
# nltk.download(['stopwords','wordnet'])

#warning
import warnings
warnings.filterwarnings('ignore')


# import spacy.cli

# spacy.cli.download("en_core_web_lg")

nlp = spacy.load("en_core_web_sm")
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
skill_pattern_path = "C:/Users/Doaa/Desktop/flaskapi/data/jz_skill_patterns.jsonl"



SKILLS_DB = pd.read_csv('C:/Users/Doaa/Desktop/flaskapi/data/skills.csv')
SKILLS_DB = list(SKILLS_DB['skills'])


TITLES_DB = pd.read_csv('C:/Users/Doaa/Desktop/flaskapi/data/jobtitles.csv')


ruler = nlp.add_pipe("entity_ruler")
ruler.from_disk(skill_pattern_path)
nlp.pipe_names




def get_skills(text):
    doc = nlp(text)
    myset = []
    subset = []
    for ent in doc.ents:
        if ent.label_ == "SKILL":
            subset.append(ent.text)
    myset.append(subset)
    return subset


def unique_skills(x):
    return list(set(x))



# !pip install PyMuPDF
import fitz
# nltk.download('stopwords')
# nltk.download('punkt')

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ''

    for page in doc:
        text = str(page.get_text())
        text = text.strip()
        text = ' '.join(text.split())
    return text

def getSkills(text):
    stop_words = set(nltk.corpus.stopwords.words('english'))
    word_tokens = nltk.tokenize.word_tokenize(text)
    found_skills = set()
    # remove the stop words
    filtered_tokens = [w for w in word_tokens if w not in stop_words]

    # remove the punctuation
    filtered_tokens = [w.lower() for w in filtered_tokens if w.isalpha()]

    # generate bigrams and trigrams (such as artificial intelligence)
    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))


    for token in filtered_tokens:
        if token.lower() in SKILLS_DB:
            found_skills.add(token)

    # we search for each bigram and trigram in our skills database
    for ngram in bigrams_trigrams:
        if ngram.lower() in SKILLS_DB:
            found_skills.add(ngram)

    return found_skills


def getTitles(text,jobtitle):
    stop_words = set(nltk.corpus.stopwords.words('english'))
    word_tokens = nltk.tokenize.word_tokenize(text)
    # remove the stop words
    filtered_tokens = [w for w in word_tokens if w not in stop_words]

    # remove the punctuation
    filtered_tokens = [w.lower() for w in filtered_tokens if w.isalpha()]

    # generate bigrams and trigrams (such as artificial intelligence)
    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))

    titles = filtered_tokens + bigrams_trigrams
    
    max_similarity_score = 0
    title = 'unknown' 
    for token in titles:
        embeddings = model.encode([token,jobtitle])
        similarity_score = cosine_similarity(embeddings)[0][1]
        if similarity_score>max_similarity_score:
            title = token
            max_similarity_score = similarity_score
    return title,max_similarity_score
# print(extract_text_from_pdf('/content/drive/MyDrive/Web_Developer_Resume_1.pdf'))




# text = extract_text_from_pdf('/content/drive/MyDrive/smartRecruiter/doaa ali ai.pdf')
# print(text)



# req_skills = 'We expect a successful candidate to be able to- design and develop software products,- create programs for heterogeneous environments (MS Windows, Unix),communicating by means ofnetwork protocols- author and maintain internal and end-user documentation. Applicants should have exposure to andprevious experience with:- programming languages: C, Perl (or Ruby, Python), and Java (or C#) - data processing technologies: XML, relational databases.We are particularly interested in candidates with experience in Oracle,DBMS application programming, J2EE architecture and applicationsdevelopment, and .NET Framework-based technologies.'
# req_skills = unique_skills(getSkills(req_skills))
# resume_skills = unique_skills(getSkills(text.lower()))



# print(f"The current Resume is {match}% matched to your requirements")

# !unzip "/content/drive/MyDrive/jobpostsGeneration.zip" -d "/content/drive/MyDrive"